{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Agentic TripAdvisor\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "### Agents\n",
    "Agents are autonomous decision-makers that use language models to determine which actions to take and in what order. Rather than following a fixed chain of operations, agents can reason about problems, choose appropriate tools, and adapt their approach based on intermediate results. Think of them as the \"brain\" that orchestrates your application's workflow.\n",
    "\n",
    "### Tools\n",
    "Tools are functions or capabilities that agents can invoke to perform specific tasks. These might include:\n",
    "-   Database queries (like MongoDB operations)\n",
    "-   API calls\n",
    "-   Web searches\n",
    "-   Calculations\n",
    "-   File operations\n",
    "\n",
    "Tools extend the LLM's capabilities beyond text generation, allowing it to interact with external systems. Each tool has a name, description, and defined input schema that helps the agent understand when and how to use it.\n",
    "\n",
    "\n",
    "### Memory\n",
    "Memory enables applications to maintain context across interactions. There are several types:\n",
    "\n",
    "- Short-term memory: Maintains conversation history within a session (stored in-memory or temporarily)\n",
    "- Long-term memory: Persists important information across sessions (often in databases like MongoDB)\n",
    "\n",
    "\n",
    "### Additional Key Concepts\n",
    "**Chains**: Sequential operations where the output of one step feeds into the next. Simpler than agents but more predictable.\n",
    "\n",
    "**Retrievers**: Components that fetch relevant documents from vector stores or databases based on semantic similarity.\n",
    "\n",
    "**Embeddings**: Vector representations of text used for semantic search in MongoDB's vector search capabilities.\n",
    "\n",
    "**Prompts**: Templates that structure inputs to the LLM, including system messages, few-shot examples, and dynamic variables."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -q -r requirements.txt",
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"MONGO_CONNECTION_STRING\"):\n",
    "    print(\"Connection string for MONGO is not set. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"MONGO_CONNECTION_STRING loaded successfully.\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"API KEY for OPENAI is not set. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY loaded successfully.\")\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    print(\"API key for Groq is not set. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")\n",
    "\n",
    "print(os.getenv(\"MONGO_CONNECTION_STRING\"))\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "print(os.environ.get(\"GROQ_API_KEY\"))"
   ],
   "id": "44207bd4121ac6d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pymongo\n",
    "\n",
    "MONGO_CONNECTION_STRING = os.environ.get(\"MONGO_CONNECTION_STRING\")\n",
    "mongo_client = pymongo.MongoClient(MONGO_CONNECTION_STRING)\n",
    "\n",
    "try:\n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"‚úÖ Connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Connection failed:\", e)\n",
    "\n",
    "db = mongo_client[\"sample_airbnb\"]\n",
    "collection = db[\"listingsAndReviews\"]\n"
   ],
   "id": "7b3c0fa5ed3ab7a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# model = \"text-embedding-3-small\"\n",
    "embedding_model_ada_002 = \"text-embedding-ada-002\"\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_embedding(text, embedding_model):\n",
    "    \"\"\"Generates vector embeddings for the given text.\"\"\"\n",
    "\n",
    "    embedding = openai_client.embeddings.create(input=[text], model=embedding_model).data[0].embedding\n",
    "    return embedding"
   ],
   "id": "b1892314e6c01933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:30:35.063889Z",
     "start_time": "2025-10-21T18:33:20.727692Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "# generate embeddings for descriptions\n",
    "import time\n",
    "\n",
    "for property_doc in collection.find(\n",
    "        {\"$and\":[\n",
    "            {\"description\": {\"$exists\": True , \"$ne\": \"\"}},\n",
    "            {\"embedding\": {\"$exists\": False}}\n",
    "        ]\n",
    "        }, {\"_id\": 1, \"description\": 1}):\n",
    "    text = property_doc.get(\"description\", \"\")\n",
    "\n",
    "    embedding = get_embedding(text, embedding_model_ada_002)\n",
    "    if embedding:\n",
    "        collection.update_one(\n",
    "            {\"_id\": property_doc[\"_id\"]},\n",
    "            {\"$set\": {\"embedding\": embedding}}\n",
    "        )\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipped doc {property_doc['_id']} (no description or embedding failed)\")\n",
    "\n",
    "    time.sleep(0.2)\n",
    "````"
   ],
   "id": "c194ee0fc80ca885"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-21T19:33:58.537500Z",
     "start_time": "2025-10-21T19:33:30.823385Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "from pymongo.operations import SearchIndexModel\n",
    "import time\n",
    "\n",
    "# Create your index model, then create the search index\n",
    "search_index_model = SearchIndexModel(\n",
    "    definition={\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"type\": \"vector\",\n",
    "                \"path\": \"embedding\",\n",
    "                \"similarity\": \"cosine\",\n",
    "                \"numDimensions\": 1536\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    name=\"vector_index\",\n",
    "    type=\"vectorSearch\"\n",
    ")\n",
    "result = collection.create_search_index(model=search_index_model)\n",
    "\n",
    "# Wait for initial sync to complete\n",
    "print(\"Polling to check if the index is ready. This may take up to a minute.\")\n",
    "predicate = None\n",
    "if predicate is None:\n",
    "    predicate = lambda index: index.get(\"queryable\") is True\n",
    "\n",
    "while True:\n",
    "    indices = list(collection.list_search_indexes(result))\n",
    "    if len(indices) and predicate(indices[0]):\n",
    "        break\n",
    "    time.sleep(5)\n",
    "print(result + \" is ready for querying.\")\n",
    "```"
   ],
   "id": "9107e35bb8e3a360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from enum import StrEnum\n",
    "import operator\n",
    "\n",
    "class Phase(StrEnum):\n",
    "    DISCOVERY = \"discovery\"\n",
    "    SERACH = \"search\"\n",
    "    REFINEMENT = \"refinement\"\n",
    "\n",
    "class DiscoveryState(BaseModel):\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "\n",
    "    # User preferences\n",
    "    location: str | None\n",
    "    travel_purpose: str | None\n",
    "    budget_min: float | None\n",
    "    budget_max: float | None\n",
    "    property_type: str | None\n",
    "\n",
    "    # Conversation control\n",
    "    search_results: list[dict] | None\n",
    "    current_phase: Phase"
   ],
   "id": "23c099ea526f6ab9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hybrid search\n",
    "\n",
    "**vector_score** measures semantic similarity between the listing and the user query, based on vector embeddings.\n",
    "\n",
    "**metadata_score** reflects listing quality using review ratings, number of reviews, and host status.\n",
    "\n",
    "**$vectorSearch** stage is used to find the listings whose description_embedding vectors are most similar to the user‚Äôs query embedding. If the parameter ```numCandidates``` is present Approximate Nearest Neighbor (ANN) Search is performed to find the top-K closest vectors quickly. If the parameter ```exact``` is present the database computes the similarity between the query vector and every stored embedding."
   ],
   "id": "15b94917c79b88e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@tool\n",
    "def search_properties_hybrid(\n",
    "        query_text: str,\n",
    "        location: str = None,\n",
    "        property_type: str = None,\n",
    "        min_price: float = None,\n",
    "        max_price: float = None,\n",
    "        top_k: int = 5\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Hybrid search combining vector similarity with metadata filters.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query_text, embedding_model_ada_002)\n",
    "\n",
    "    match_conditions = []\n",
    "\n",
    "    if location:\n",
    "        match_conditions.append({\n",
    "            \"$or\": [\n",
    "                {\"address.market\": {\"$regex\": location, \"$options\": \"i\"}},\n",
    "                {\"address.country\": {\"$regex\": location, \"$options\": \"i\"}},\n",
    "                {\"address.suburb\": {\"$regex\": location, \"$options\": \"i\"}}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    if property_type:\n",
    "        match_conditions.append({\"property_type\": property_type})\n",
    "\n",
    "    if min_price or max_price:\n",
    "        price_condition = {}\n",
    "        if min_price:\n",
    "            price_condition[\"$gte\"] = min_price\n",
    "        if max_price:\n",
    "            price_condition[\"$lte\"] = max_price\n",
    "        match_conditions.append({\"price\": price_condition})\n",
    "\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"property_vector_index\",\n",
    "                \"path\": \"description_embedding\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"numCandidates\": 200,\n",
    "                \"limit\": 50\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if match_conditions:\n",
    "        pipeline.append({\"$match\": {\"$and\": match_conditions}})\n",
    "\n",
    "    pipeline.extend([\n",
    "        {\n",
    "            \"$addFields\": {\n",
    "                \"vector_score\": {\"$meta\": \"vectorSearchScore\"},\n",
    "                \"metadata_score\": {\n",
    "                    \"$add\": [\n",
    "                        {\"$multiply\": [{\"$ifNull\": [\"$review_scores.review_scores_rating\", 0]}, 0.2]},\n",
    "                        {\"$min\": [{\"$multiply\": [{\"$ifNull\": [\"$number_of_reviews\", 0]}, 0.1]}, 10]},\n",
    "                        {\"$cond\": [{\"$eq\": [\"$host.host_is_superhost\", True]}, 5, 0]}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$addFields\": {\n",
    "                \"final_score\": {\n",
    "                    \"$add\": [\n",
    "                        {\"$multiply\": [\"$vector_score\", 0.6]},\n",
    "                        {\"$multiply\": [\"$metadata_score\", 0.4]}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\"$sort\": {\"final_score\": -1}},\n",
    "        {\"$limit\": top_k}\n",
    "    ])\n",
    "\n",
    "    # Build projection\n",
    "    fields = [\"_id\", \"name\", \"property_type\", \"bedrooms\", \"beds\", \"price\", \"address\", \"amenities\", \"review_scores\",\n",
    "              \"final_score\", \"vector_score\", \"metadata_score\"]\n",
    "    projection = {field: 1 for field in fields}\n",
    "    pipeline.extend({\"$project\": projection})\n",
    "\n",
    "    # Execute the search\n",
    "    try:\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Vector search failed: {str(e)}\"}]"
   ],
   "id": "6431cacc6a1e2a3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm_discovery = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")\n",
    "llm_extract_preferences = init_chat_model(\"openai/gpt-oss-120b\", model_provider=\"groq\")"
   ],
   "id": "c9ecbe6ef7e31be9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def vector_search(query_text, location=None, limit=5 ):\n",
    "    \"\"\"\n",
    "    Executes a vector similarity search in MongoDB Atlas using a defined search index.\n",
    "    \"\"\"\n",
    "    # Build projection\n",
    "    fields = [\"_id\", \"name\", \"property_type\", \"bedrooms\", \"beds\", \"price\", \"address\", \"amenities\", \"review_scores\"]\n",
    "    projection = {field: 1 for field in fields}\n",
    "    projection[\"score\"] = {\"$meta\": \"vectorSearchScore\"}\n",
    "\n",
    "    query_vector = get_embedding(query_text, embedding_model_ada_002)\n",
    "\n",
    "    # Build aggregation pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_vector,\n",
    "                \"path\": \"embedding\",\n",
    "                \"limit\": limit,\n",
    "                \"numCandidates\": 100\n",
    "            }\n",
    "        },\n",
    "        {\"$project\": projection}\n",
    "    ]\n",
    "\n",
    "    # Add location filter if provided\n",
    "    # case-insensitive substring search\n",
    "    if location:\n",
    "        pipeline.insert(1, {\n",
    "            \"$match\": {\n",
    "                \"$or\": [\n",
    "                    {\"address.market\": {\"$regex\": location, \"$options\": \"i\"}},\n",
    "                    {\"address.country\": {\"$regex\": location, \"$options\": \"i\"}},\n",
    "                    {\"address.suburb\": {\"$regex\": location, \"$options\": \"i\"}}\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    pipeline.append({\"$project\": projection})\n",
    "\n",
    "    # Execute the search\n",
    "    try:\n",
    "        results = list(collection.aggregate(pipeline))\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Vector search failed: {str(e)}\"}]\n",
    "\n",
    "    return results"
   ],
   "id": "7aff55020d2bc101",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DISCOVERY_SYSTEM_PROMPT = \"\"\"You are a friendly and helpful AirBnB property discovery assistant.\n",
    "\n",
    "Your goal is to understand the user's travel needs through natural conversation, then help them find the perfect property.\n",
    "\n",
    "DISCOVERY PHASE GUIDELINES:\n",
    "1. Ask questions naturally, one at a time (don't overwhelm with multiple questions)\n",
    "2. Be conversational and warm, not robotic\n",
    "3. Adapt questions based on previous answers\n",
    "4. Once you have enough information to search, synthesize their preferences into a search query\n",
    "\n",
    "KEY INFORMATION TO GATHER:\n",
    "- Location (city, neighborhood, country)\n",
    "- Travel purpose (work, leisure, family, etc.)\n",
    "- Budget range per night\n",
    "- Property preferences (entire place, private room, shared)\n",
    "- Group composition (solo, couple, family, group size)\n",
    "- Must-have amenities\n",
    "- Desired vibe/atmosphere\n",
    "\n",
    "CURRENT STATE: You are gathering information. When you have location + at least 3 other preference points, you can proceed to search.\n",
    "\n",
    "Be enthusiastic and make the user excited about their trip!\n",
    "\"\"\""
   ],
   "id": "9a2033182a2429fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EXTRACTION_PROMPT  = \"\"\"Based on the conversation, extract the following information in a structured format:\n",
    "\n",
    "    EXTRACT (use \"null\" if not mentioned):\n",
    "    - location: The city/country/neighborhood they want to stay\n",
    "    - travel_purpose: Why they're traveling (work, leisure, family visit, etc.)\n",
    "    - budget_min: Minimum price per night (number only)\n",
    "    - budget_max: Maximum price per night (number only)\n",
    "    - property_type: Type preference (entire home/apt, private room, shared room)\n",
    "    - group_composition: Who's traveling (solo, couple, family with X kids, group of X, etc.)\n",
    "    - amenities: List of must-have amenities mentioned\n",
    "    - vibe_preference: Desired atmosphere/vibe of property or neighborhood\n",
    "\n",
    "    Return ONLY a JSON object with these fields. Be precise and extract only explicitly mentioned information.\n",
    "\"\"\""
   ],
   "id": "908f75af35beef1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "def discovery_agent(state: DiscoveryState) -> DiscoveryState:\n",
    "    \"\"\"\n",
    "    Agent node asking questions and gathering preferences.\n",
    "    \"\"\"\n",
    "    messages = [SystemMessage(content=DISCOVERY_SYSTEM_PROMPT)] + state.messages\n",
    "    current_phase = state.current_phase\n",
    "\n",
    "    # Check if we have enough information\n",
    "    info_collected = sum([\n",
    "        bool(state.location),\n",
    "        bool(state.travel_purpose),\n",
    "        bool(state.property_type),\n",
    "        bool(state.budget_min),\n",
    "        bool(state.budget_max),\n",
    "    ])\n",
    "\n",
    "    # Add context about what we know\n",
    "    context_msg = f\"\\nInformation collected so far: {info_collected}/5 key points.\"\n",
    "    if state.location:\n",
    "        context_msg += f\"\\n- Location: {state.location}\"\n",
    "    if state.travel_purpose:\n",
    "        context_msg += f\"\\n- Purpose: {state.travel_purpose}\"\n",
    "    if state.property_type:\n",
    "        context_msg += f\"\\n- Property type: {state.property_type}\"\n",
    "    if state.budget_min:\n",
    "        context_msg += f\"\\n- Budget min: {state.budget_min}\"\n",
    "    if state.budget_max:\n",
    "        context_msg += f\"\\n- Budget max: {state.budget_max}\"\n",
    "\n",
    "    print(\"info collected:\", info_collected)\n",
    "\n",
    "    if info_collected >= 3:\n",
    "        context_msg += \"\\n\\nYou have enough information to search! Synthesize their preferences and offer to find properties.\"\n",
    "        current_phase = Phase.SERACH\n",
    "\n",
    "    messages.append(SystemMessage(content=context_msg))\n",
    "\n",
    "    # Generate response\n",
    "    response = llm_discovery.invoke(messages)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"current_phase\": current_phase\n",
    "    }"
   ],
   "id": "46b108b7ea3df75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def extract_preferences(state: DiscoveryState) -> DiscoveryState:\n",
    "    \"\"\"\n",
    "    Extract structured preferences from conversation using LLM.\n",
    "    \"\"\"\n",
    "    messages = state.messages\n",
    "\n",
    "    result = llm_extract_preferences.invoke(messages + [HumanMessage(content=EXTRACTION_PROMPT)])\n",
    "\n",
    "    # Parse the JSON response (simplified - add error handling in production)\n",
    "    import json\n",
    "    try:\n",
    "        extracted = json.loads(result.content)\n",
    "\n",
    "        return {\n",
    "            \"location\": extracted.get(\"location\"),\n",
    "            \"travel_purpose\": extracted.get(\"travel_purpose\"),\n",
    "            \"property_type\": extracted.get(\"property_type\"),\n",
    "            \"budget_min\": extracted.get(\"budget_min\"),\n",
    "            \"budget_max\": extracted.get(\"budget_max\")\n",
    "        }\n",
    "    except:\n",
    "        return {}"
   ],
   "id": "fbbd3eb974ebdc89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def create_discovery_graph():\n",
    "    \"\"\"Build the LangGraph workflow for new user discovery.\"\"\"\n",
    "\n",
    "    workflow = StateGraph(DiscoveryState)\n",
    "\n",
    "    # Add nodes\n",
    "    workflow.add_node(\"discovery\", discovery_agent)\n",
    "    workflow.add_node(\"extract_preferences\", extract_preferences)\n",
    "\n",
    "\n",
    "    workflow.set_entry_point(\"discovery\")\n",
    "    workflow.add_edge(\"discovery\", \"extract_preferences\")\n",
    "    workflow.add_edge(\"extract_preferences\", END)\n",
    "\n",
    "    return workflow.compile()"
   ],
   "id": "27e85b7961441b7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "app = create_discovery_graph()\n",
    "\n",
    "# Initialize state\n",
    "initial_state = DiscoveryState(\n",
    "    messages = [],\n",
    "    location = None,\n",
    "    property_type= None,\n",
    "    travel_purpose = None,\n",
    "    budget_min = None,\n",
    "    budget_max = None,\n",
    "    search_results =  None,\n",
    "    current_phase = Phase.DISCOVERY\n",
    ")\n",
    "\n",
    "# Run the conversation\n",
    "print(\"üè† AirBnB Discovery Agent\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Agent: Hi! I'm here to help you find the perfect place to stay. Where would you like to go?\\n\")\n",
    "\n",
    "state = initial_state\n",
    "while True:\n",
    "        # Get user input\n",
    "        # example: \"Hi! I'm looking for a place to stay in Barcelona\"\n",
    "        user_input = input(\"You: \").strip()\n",
    "\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            print(\"\\nAgent: Thanks for chatting! Your preferences have been saved. Happy travels! üåç\")\n",
    "            break\n",
    "\n",
    "        if not user_input:\n",
    "            continue\n",
    "\n",
    "        # Add user message to state\n",
    "        state.messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "        # Run agent\n",
    "        try:\n",
    "            # Execute graph\n",
    "            result = app.invoke(state)\n",
    "\n",
    "            new_messages = result.pop(\"messages\", [])\n",
    "            other_updates = result\n",
    "            state = state.model_copy(update=other_updates)\n",
    "            state = state.model_copy(update={\"messages\": state.messages + new_messages})\n",
    "\n",
    "            print(\"state current phase:\", state.current_phase)\n",
    "\n",
    "            # Display agent response\n",
    "            last_message = state.messages[-1]\n",
    "            if isinstance(last_message, AIMessage):\n",
    "                print(f\"\\nAgent: {last_message.content}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAgent: I encountered an error: {str(e)}\")\n",
    "            print(\"Let's continue our conversation.\\n\")\n",
    "\n",
    "\n",
    "print(result)"
   ],
   "id": "626edfc8021ee90d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "graph = create_discovery_graph()\n",
    "print(graph.get_graph().draw_mermaid())\n",
    "# try at https://mermaid.live/\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "1d8135b9cd9d9664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 1\n",
    "\n",
    "Modify the state of the LangGraph agent that searches listings to include new contextual search criteria.\n",
    "\n",
    "Add the following fields to the agent‚Äôs state and ensure they are incorporated into the exploration step:\n",
    "group_composition, amenities, vibe_preference."
   ],
   "id": "5a1603f0c6d535bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 2\n",
    "\n",
    "Build an intelligent movie search agent using the sample_mflix MongoDB dataset.\n",
    "The agent should use Retrieval-Augmented Generation (RAG) and short-term memory to help users find a specific movie through natural-language dialogue. The user interacts conversationally. The agent should remember the context from previous turns, retrieve relevant documents from MongoDB, combine retrieval results with LLM reasoning, and produce a refined natural-language answer."
   ],
   "id": "eb72de852908b482"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise 3\n",
    "\n",
    "Design an intelligent agent that converts natural-language questions into executable SQL queries, retrieves results from a relational database, and stores user feedback on the generated queries and responses in a MongoDB collection.\n",
    "\n",
    "The agent should: Execute the query and summarize the results in natural language; Allow the user to rate the response or correct the generated query. Add conditional edges: if a question is found in the long term histoy, the agen should use the history database to return the results.\n",
    "\n",
    "\n"
   ],
   "id": "8568b940d857b879"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
