{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text-To-Sql Demo",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!git clone https://github.com/iuliabanu/BDNSV.git",
   "id": "2199be82c4b03209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%cd /content/BDNSVW/TextToSql",
   "id": "e9e1386930156eed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -q -r requirements.txt",
   "id": "10ef8127427b3ced",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test databases.\n",
    "\n",
    "Load test_databases from BIRD benchmark from [url_bird](\"https://bird-bench.oss-cn-beijing.aliyuncs.com/dev.zip\").\n",
    "\n",
    "We will use formula_1 and student_club databases. You can inspect database description for [formula_1](data/formula_1/) and\n",
    "for [student_club](data/student_club).\n",
    "\n",
    "Test some basic interaction wiht the databases using [sqlalchemy](https://www.sqlalchemy.org/) or langchain_community.utilities.SQLDatabase.\n",
    "You can install all the requirments by running:\n",
    "\n",
    "pip install -r requirements.txt"
   ],
   "id": "df3c6ad1a566a1bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sqlalchemy import create_engine, inspect, text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "\n",
    "# The path to SQLite file\n",
    "db_path = \"data/formula_1/formula_1.sqlite\"\n",
    "\n",
    "# SQLAlchemy engine to connect to the database file\n",
    "engine: Engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "\n",
    "# Run a query using the engine\n",
    "query = \"SELECT * FROM drivers LIMIT 10;\"\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(query))\n",
    "    rows = result.fetchall()\n",
    "\n",
    "# Print results\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Inspect schema\n",
    "inspector = inspect(engine)\n",
    "print(inspector.get_table_names())\n",
    "\n"
   ],
   "id": "8cb957bfe67678a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "db = SQLDatabase(engine=engine)"
   ],
   "id": "bd4599a854ee979a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pydantic BaseModes\n",
    "\n",
    "Pydantic **BaseModel**s are Python classes that define a structured way to handle, validate, and parse data. They are fundamental to the Pydantic library, a data validation and settings management tool that relies on Python type hints.\n",
    "\n",
    "Essentially, a BaseModel allows you to define a schema for data, and Pydantic ensures that data that is passed to that model conforms to the specified types and constraints.\n",
    "\n",
    "Key Features\n",
    "- Type validation: Ensures that data matches the expected types.\n",
    "- Automatic parsing: Converts input types (e.g., strings to integers).\n",
    "- Serialization: Easily convert models to and from JSON/dictionaries.\n",
    "- Error reporting: Gives detailed error messages when validation fails.\n",
    "\n",
    "Use cases:\n",
    "- FastAPI request/response models\n",
    "- Configuration files\n",
    "- Data validation in ETL pipelines\n",
    "- Typed data structures in ML projects\n",
    "\n",
    "We will define a State - Model for the base functionalities of a text-to-sql pipeline: the nl question, the query generated by the LLM model, the result of the execution of the query and the final answer returned the user."
   ],
   "id": "d29521cdd27b441b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class State(BaseModel):\n",
    "    question: str\n",
    "    query: str = \"\"\n",
    "    result: str = \"\"\n",
    "    answer: str = \"\""
   ],
   "id": "c13dd597eab57993",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Groq\n",
    "\n",
    "[Groq project](https://groq.com/) is foucuse on AI inference acceleration. Groq solutions are based on the Language Processing Unit (LPU), a new category of processor, built specifically for running AI models fast and efficiently. GroqCard card used in both cloud and on-premise setups. Groq Cloud is a cloud-hosted AI inference platform powered by LPUs. It’s designed to serve generative AI models across text, audio, and vision with minimal latency and high throughput.\n",
    "\n",
    "### Langchain\n",
    "\n",
    "[LangChain](https://www.langchain.com/) is an open-source framework designed to build applications using Large Language Models (LLMs) in a modular and scalable way. It simplifies the creation of chains—structured sequences of operations involving LLMs, tools, and data sources.\n",
    "\n",
    "[Core concepts](https://python.langchain.com/docs/concepts/)\n",
    "\n",
    "- **Chains**: Sequences of steps (e.g., prompt → LLM → output → tool) that define how data flows through an application.\n",
    "- **Agents**: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tools.\n",
    "- Tools: A function (e.g., search engines, calculators, databases)  with an associated schema defining the function's name, description, and the arguments it accepts.\n",
    "- Retriever: A component that returns relevant documents from a knowledge base in response to a query.\n",
    "- Memory: Information about a conversation that is persisted so that it can be used in future conversations.\n",
    "\n",
    "Use cases\n",
    "- Text-to-SQL and database querying\n",
    "- Chatbots and virtual assistants\n",
    "- Document Q&A and summarization\n",
    "- Code generation and debugging\n",
    "- Autonomous agents and workflows"
   ],
   "id": "d3b5e305ba7475ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    print(\"API key for Groq is not set. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")\n"
   ],
   "id": "850499a15d40f673",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Prompt templates** help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "**Tool chaining** Many AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language. But what about cases where we want a model to also interact directly with systems, such as databases or an API? These systems often have a particular input schema; for example, APIs frequently have a required payload structure. This need motivates the concept of tool calling. You can use tool calling to request model responses that match a particular schema."
   ],
   "id": "74a089e1f5bee71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_message = \"\"\"\n",
    "Given an input question, create a syntactically correct {dialect} query to\n",
    "run to help find the answer. Unless the user specifies in his question a\n",
    "specific number of examples they wish to obtain, always limit your query to\n",
    "at most {top_k} results. You can order the results by a relevant column to\n",
    "return the most interesting examples in the database.\n",
    "\n",
    "Never query for all the columns from a specific table, only ask for a the\n",
    "few relevant columns given the question.\n",
    "\n",
    "Pay attention to use only the column names that you can see in the schema\n",
    "description. Be careful to not query for columns that do not exist. Also,\n",
    "pay attention to which column is in which table.\n",
    "\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"Question: {input}\"\n",
    "\n",
    "query_prompt_template = ChatPromptTemplate(\n",
    "    [(\"system\", system_message), (\"user\", user_prompt)]\n",
    ")\n",
    "\n",
    "for message in query_prompt_template.messages:\n",
    "    message.pretty_print()"
   ],
   "id": "c0eca3d121091f03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class QueryOutput(BaseModel):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "\n",
    "    query: str = Field(description=\"Syntactically valid SQL query.\")\n",
    "\n",
    "\n",
    "def write_query(state: State):\n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "\n",
    "    prompt = query_prompt_template.invoke(\n",
    "        {\n",
    "            \"dialect\": db.dialect,\n",
    "            \"top_k\": 1,\n",
    "            \"table_info\": db.get_table_info(),\n",
    "            \"input\": state.question\n",
    "        }\n",
    "    )\n",
    "\n",
    "#invoke the llm\n",
    "    #result = llm.invoke(prompt)\n",
    "    #print(result)\n",
    "\n",
    "    llm_with_tools = llm.bind_tools([QueryOutput])\n",
    "    msg = llm_with_tools.invoke(prompt)\n",
    "    state.query = msg.tool_calls[0][\"args\"][\"query\"]\n",
    "    return msg.tool_calls[0][\"args\"]\n",
    "\n"
   ],
   "id": "c49a781bb3862839",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "state = State(question=\"How many drivers are there?\")\n",
    "write_query(state)"
   ],
   "id": "7a972cb23be5879",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool, QuerySQLCheckerTool\n",
    "\n",
    "def execute_query(state: State):\n",
    "    \"\"\"Execute SQL query.\"\"\"\n",
    "    checker_tool = QuerySQLCheckerTool(db=db, llm=llm)\n",
    "    check_result = checker_tool.invoke(state.query)\n",
    "    if \"error\" in check_result.lower():\n",
    "        return {\"result\": f\"Invalid SQL query: {check_result}\"}\n",
    "\n",
    "    execute_query_tool = QuerySQLDatabaseTool(db=db)\n",
    "    exec_result = execute_query_tool.invoke(state.query)\n",
    "    state.result = exec_result\n",
    "    return {\"result\": execute_query_tool.invoke(state.query)}"
   ],
   "id": "94e108ae2aa5dcb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "execute_query(state)",
   "id": "4bac29c7325c7fe7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_answer(state: State):\n",
    "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
    "    prompt = (\n",
    "        \"Given the following user question, corresponding SQL query, \"\n",
    "        \"and SQL result, answer the user question.\\n\\n\"\n",
    "        f\"Question: {state.question}\\n\"\n",
    "        f\"SQL Query: {state.query}\\n\"\n",
    "        f\"SQL Result: {state.result}\"\n",
    "    )\n",
    "    print(f\"prompt: {prompt}\")\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"answer\": response.content}"
   ],
   "id": "f21a8ee3eb6ecabc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "generate_answer(state)",
   "id": "d7f4a4705f4fbe47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A StateGraph object defines the structure of an agent as a \"state machine\". Each node can receive the current State as input and output an update to the state.",
   "id": "8968a6b7fc3005e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence(\n",
    "    [write_query, execute_query, generate_answer]\n",
    ")\n",
    "graph_builder.add_edge(START, \"write_query\")\n",
    "graph = graph_builder.compile()"
   ],
   "id": "c7b14667bfcbda70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "539d862d0a77732d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "initial_state = State(question=\"How many drivers are there?\")\n",
    "\n",
    "for step in graph.stream({\"question\": \"How many drivers are there?\"}, stream_mode=\"updates\"):\n",
    "    print(step)\n"
   ],
   "id": "de6423a6374b8191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "# Suppose your JSON is stored in a file\n",
    "qustions_path = \"data/dev.json\"\n",
    "\n",
    "with open(qustions_path, \"r\") as f:\n",
    "    data = json.load(f)  # this should be a list of dicts\n",
    "\n",
    "\n",
    "def filter_questions(data, db_id, difficulty):\n",
    "    return [\n",
    "        (item[\"question\"], item[\"SQL\"])\n",
    "        for item in data\n",
    "        if item.get(\"db_id\") == db_id and item.get(\"difficulty\") == difficulty\n",
    "    ]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "questions = filter_questions(data, db_id=\"formula_1\", difficulty=\"simple\")\n",
    "print(f\"questions: {questions}\")"
   ],
   "id": "1f6a0f4b788929ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Eval(BaseModel):\n",
    "    question: str\n",
    "    query_ground_truth: str\n",
    "    query_llm: str\n",
    "    score_ea: int\n",
    "    score_cm: int"
   ],
   "id": "22a5ca84d67e93b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_eval_list(filtered_list, max_questions, graph):\n",
    "    evals = []\n",
    "    skip = []\n",
    "\n",
    "    for i, (question, ground_truth) in enumerate(filtered_list):\n",
    "        if i >= max_questions:\n",
    "            break\n",
    "        try:\n",
    "            state = State(question=question)\n",
    "\n",
    "            result = graph.invoke(state)\n",
    "            llm_query = result.get(\"query\", \"\")\n",
    "\n",
    "            evals.append(\n",
    "                Eval(\n",
    "                    question=question,\n",
    "                    query_ground_truth=ground_truth,\n",
    "                    query_llm=llm_query,\n",
    "                    score_ea=0,\n",
    "                    score_cm=0\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping question due to error: {e}\")\n",
    "            skip.append(\n",
    "                Eval(question=question,\n",
    "                     query_ground_truth=ground_truth,\n",
    "                     query_llm=\"\",\n",
    "                     score_ea=0,\n",
    "                     score_cm=0\n",
    "                     )\n",
    "            )\n",
    "            continue\n",
    "    return evals"
   ],
   "id": "de4ed0132eeffcea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
