{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# MongoDb for Agents and RAG.\n",
    "\n",
    "**Retrieval-augmented generation (RAG)** is an architecture used to augment large language models (LLMs) with additional data so that they can generate more accurate responses.\n",
    "\n",
    "**Memory**: Agents can remember previous interactions. Memory can be short-term (for the current session) or long-term (persisted across sessions).\n",
    "\n",
    "**Tools**: Agents can use **vector search** as a tool to retrieve relevant information and implement RAG. The vector search returns documents whose embeddings are closest in distance to the embedding the user's query."
   ],
   "id": "98e51256fd1632be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install -q -r requirements.txt",
   "id": "fb46cd5c1a266cc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We will use:\n",
    "- [Mongo Atlas](https://cloud.mongodb.com) for the sample database sample_mflix.\n",
    "- [OpenAi](https://platform.openai.com/) or [Voyageai](https://www.voyageai.com/) for embeddings models. The collection sample_mfix.embedded_movies  has embedding generated wiht both OpneAi an Voyage models.\n",
    "- [Groq](https://console.groq.com/) for LLM models.\n",
    "\n",
    "Set MONGO_CONNECTION_STRING, OPENAI_API_KEY and GROQ_API_KEY\n",
    "\n",
    "Follow the steps in:\n",
    "\n",
    "- [Mongo docs-create database](https://www.mongodb.com/resources/products/fundamentals/create-database)\n",
    "\n",
    "- [Mongo docs-cluster setup](https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup)"
   ],
   "id": "21b3db105bd8a468"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"MONGO_CONNECTION_STRING\"):\n",
    "    print(\"Connection string for MONGO is not set. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"MONGO_CONNECTION_STRING loaded successfully.\")\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"API KEY for OPENAI is not set. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY loaded successfully.\")\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    print(\"API key for Groq is not set. Please check your .env file.\")\n",
    "else:\n",
    "    print(\"API key loaded successfully.\")\n",
    "\n"
   ],
   "id": "872af2ce816c34a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Connect to MongoDb Cluster\n",
    "\n",
    "Connect to the MongoDB and initialize with mongo_client db = sample_mflix and collection = embedded_movies."
   ],
   "id": "ccb9b77887bce99a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pymongo\n",
    "\n",
    "MONGO_CONNECTION_STRING = os.environ.get(\"MONGO_CONNECTION_STRING\")\n",
    "mongo_client = pymongo.MongoClient(MONGO_CONNECTION_STRING)\n",
    "\n",
    "try:\n",
    "    mongo_client.admin.command('ping')\n",
    "    print(\"✅ Connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Connection failed:\", e)\n",
    "\n",
    "db = mongo_client[\"sample_mflix\"]\n",
    "embedded_movies_collection = db[\"embedded_movies\"]"
   ],
   "id": "5ba01a6c44cfa7c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test embedding models with openai_client\n",
    "Test embedding models from OpenAI. We can check the setting of the organization linked to the API-KEY."
   ],
   "id": "51b9e541ce9f7e72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "model_3small = \"text-embedding-3-small\"\n",
    "model_ada002 = \"text-embedding-ada-002\"\n",
    "openai_client = OpenAI()\n",
    "\n",
    "print(openai_client.organization)\n",
    "print(openai_client.models.list())\n",
    "\n",
    "# Define a function to generate embeddings\n",
    "def get_embedding(text, model):\n",
    "   \"\"\"Generates vector embeddings for the given text.\"\"\"\n",
    "\n",
    "   embedding = openai_client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "   return embedding\n",
    "\n",
    "# Generate an embedding\n",
    "embedding_3small = get_embedding(text = \"foo\", model = model_3small)\n",
    "print(embedding_3small)\n",
    "\n",
    "\n",
    "embedding_ada002 = get_embedding(text = \"foo\", model = model_ada002)\n",
    "print(embedding_ada002)"
   ],
   "id": "6f2c37f20f4094f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create an index for sample_mfix.embedded_movies\n",
    "\n",
    "Parameters:\n",
    "- **`fields`**: A list describing which field(s) in the documents should be indexed.\n",
    "- **`type`**: Set to `\"vector\"` to indicate this is a vector search index.\n",
    "- **`path`**: The document field containing the embedding vectors (here, `\"plot_embedding\"`).\n",
    "- **`similarity`**: The distance metric used to compare vectors—commonly `\"cosine\"`, `\"dotProduct\"`, or `\"euclidean\"`.\n",
    "- **`numDimensions`**: The dimensionality of the embeddings (here, `1536`, matching the model output).\n",
    "\n",
    "The index is given a custom **name** (`\"vector_index\"`) and a **type** (`\"vectorSearch\"`). The default value for type is 'search'.\n",
    "\n",
    "We poll MongoDB at regular intervals to check when the index becomes *queryable*.\n",
    "\n",
    "After submitting the index definition with `create_search_index()`, the script polls MongoDB at regular intervals to check when the index becomes *queryable*. Once the index has fully synchronized, a confirmation message is printed indicating that it’s ready for use.\n",
    "\n",
    "\n",
    "[mongo docs - create_search_index](https://www.mongodb.com/docs/php-library/current/reference/method/MongoDBCollection-createSearchIndex/)\n"
   ],
   "id": "a432e7e592b2ba91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pymongo.operations import SearchIndexModel\n",
    "import time\n",
    "\n",
    "# Create the index model, then create the search index\n",
    "search_index_model = SearchIndexModel(\n",
    "  definition = {\n",
    "    \"fields\": [\n",
    "      {\n",
    "        \"type\": \"vector\",\n",
    "        \"path\": \"plot_embedding\",\n",
    "        \"similarity\": \"cosine\",\n",
    "        \"numDimensions\": 1536\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  name=\"vector_index_plot\",\n",
    "  type=\"vectorSearch\"\n",
    ")\n",
    "result = embedded_movies_collection.create_search_index(model=search_index_model)\n",
    "\n",
    "# Wait for initial sync to complete\n",
    "print(\"Polling to check if the index is ready. This may take up to a minute.\")\n",
    "predicate = lambda index: index.get(\"queryable\") is True\n",
    "\n",
    "while True:\n",
    "  indices = list(embedded_movies_collection.list_search_indexes(result))\n",
    "  if len(indices) and predicate(indices[0]):\n",
    "    break\n",
    "  time.sleep(5)\n",
    "print(result + \" is ready for querying.\")"
   ],
   "id": "c43eadd68cf582bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Test the search index\n",
    "\n",
    "Test the search index created above. We get the embeddings from NL querys from OpenAi API and then search the sample_mflix.embedded_movies.\n",
    "\n",
    "In MongoDB, an **aggregation pipeline** is a sequence of stages that process and transform documents step by step.\n",
    "Each stage performs an operation (such as filtering, projecting, grouping, or searching) and passes the results to the next stage — similar to a data processing pipeline.\n",
    "\n",
    "Pipelines are defined as Python lists of dictionaries, where each dictionary represents a stage (e.g., `$match`, `$project`, `$group`, `$vectorSearch`). This approach allows complex queries, transformations, and search operations to be performed efficiently on the database side.\n"
   ],
   "id": "5be7ab78d7ad2c70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate embedding for the search query\n",
    "query1_embedding = get_embedding(text = \"Show me movies that tell the story of artificial intelligence becoming dangerous.\", model = model_ada002 )\n",
    "query2_embedding = get_embedding(text = \"Find a movie about dinosaurs coming back to life.\", model = model_ada002 )\n",
    "query3_embedding = get_embedding(text = \"Which movies are about time travel or alternate realities?\", model = model_ada002 )"
   ],
   "id": "8d2d5f7b46d5f74b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_vector_search(collection, query_vector, index_name,\n",
    "                      path, fields, limit=5, exact=True, ):\n",
    "    \"\"\"\n",
    "    Executes a vector similarity search in MongoDB Atlas using a defined search index.\n",
    "    \"\"\"\n",
    "    # Build projection dynamically\n",
    "    projection = {field: 1 for field in fields}\n",
    "    projection[\"_id\"] = 0\n",
    "    projection[\"score\"] = {\"$meta\": \"vectorSearchScore\"}\n",
    "\n",
    "    # Build aggregation pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": index_name,\n",
    "                \"queryVector\": query_vector,\n",
    "                \"path\": path,\n",
    "                \"exact\": exact,\n",
    "                \"limit\": limit\n",
    "            }\n",
    "        },\n",
    "        {\"$project\": projection}\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = list(collection.aggregate(pipeline))\n",
    "    return results\n"
   ],
   "id": "ed2c67d38049e63a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(run_vector_search(collection = embedded_movies_collection, query_vector = query1_embedding, index_name=\"vector_index_plot\",\n",
    "                      path=\"plot_embedding\", fields = [\"title\", \"plot\"], limit=5, exact=True, ))\n"
   ],
   "id": "25b734ed69c9a137",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Code Example\n",
    "\n",
    "```python\n",
    "# Sample vector search pipeline\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"index\": \"vector_index\",\n",
    "            \"queryVector\": query_embedding,\n",
    "            \"path\": \"plot_embedding\",\n",
    "            \"exact\": True,\n",
    "            \"limit\": 5\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"_id\": 0,\n",
    "            \"title\": 1,\n",
    "            \"plot\": 1,\n",
    "            \"score\": {\n",
    "                \"$meta\": \"vectorSearchScore\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute the search\n",
    "results = collection.aggregate(pipeline)\n",
    "\n",
    "# Print results\n",
    "for i in results:\n",
    "    print(i)\n",
    "```"
   ],
   "id": "cd4dfd46df849145"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agents and Tools",
   "id": "8978e4780f5f5626"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def check_mflix_similarity(plot: str) -> str:\n",
    "    \"\"\"\n",
    "    Check if the generated plot is similar\n",
    "    to an existing movie plot in the MFlix vector store.\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(text = plot, model = model_ada002)\n",
    "    results = run_vector_search(collection = embedded_movies_collection, query_vector = query_embedding, index_name=\"vector_index_plot\",\n",
    "                      path=\"plot_embedding\", fields = [\"title\", \"plot\"], limit=5, exact=True, )\n",
    "    return results\n",
    "\n",
    "\n"
   ],
   "id": "dc2c0379f4ccf9bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "plot_finder_system_prompt = \"\"\"\n",
    "Your role is to read the users plot and help the user check mflix similarity.\n",
    "\n",
    "Return the plot found by similarity check or a question helping the user refine the plot for further serches.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", plot_finder_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "33a3bd78be3e2179",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "\n",
    "toolkit = [check_mflix_similarity]\n",
    "llm = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")\n",
    "agent = create_openai_tools_agent(llm, toolkit, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=toolkit, verbose=True)\n",
    "result = agent_executor.invoke({\"input\": \"A man trapped on Mars.\"})\n",
    "\n",
    "print(result['output'])\n",
    "print(result)"
   ],
   "id": "953b913e5814696",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = agent_executor.invoke({\"input\": \"Story of the great fire of 1871.\"})\n",
    "\n",
    "print(result['output'])\n",
    "print(result)"
   ],
   "id": "492069b049ac14b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Short memory\n",
    "\n",
    "The next steps will create a database langchain_db and a collection langchain_db.rag_with_memory that will act as a **short-term memory store** for previous user–LLM interactions.\n",
    "This setup enables the system to retain recent context, conversation history across multiple queries, improving the coherence of responses."
   ],
   "id": "205b11c78c11e50a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Use the text-embedding-ada-002 or voyage-3-large embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create the vector store\n",
    "vector_store_history = MongoDBAtlasVectorSearch.from_connection_string(\n",
    "    connection_string=MONGO_CONNECTION_STRING,\n",
    "    embedding=embedding_model,\n",
    "    namespace=\"langchain_db.rag_with_memory\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "vector_store_history.create_vector_search_index(\n",
    "   dimensions = 1536\n",
    ")"
   ],
   "id": "b72ca10618f18f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "query = {\n",
    "    \"awards.wins\": {\n",
    "        \"$gt\": 8,\n",
    "        \"$lt\": 10\n",
    "    }\n",
    "}\n",
    "\n",
    "projection = {\"plot\": 1, \"title\": 1, \"directors\": 1, \"_id\": 0}\n",
    "\n",
    "# Fetch the data\n",
    "movie_data = embedded_movies_collection.find(query, projection).limit(1000) # Limiting for a manageable example\n",
    "\n",
    "# --- Create the list of LangChain Documents ---\n",
    "docs = []\n",
    "\n",
    "for movie in movie_data:\n",
    "    # 1. Create the content (the plot)\n",
    "    content = movie.get('plot', '')\n",
    "\n",
    "    # 2. Create the metadata (the title and source)\n",
    "    # Adding the directors list to the metadata is useful for context/retrieval\n",
    "    metadata = {\n",
    "        \"title\": movie.get('title'),\n",
    "        \"source\": f\"MFlix Movie Plot: {movie.get('title')}\",\n",
    "        \"directors\": movie.get('directors')\n",
    "    }\n",
    "\n",
    "    # 3. Create the Document object and append it to the list\n",
    "    if content: # Ensure we only add documents with a plot\n",
    "        doc = Document(page_content=content, metadata=metadata)\n",
    "        docs.append(doc)\n",
    "\n",
    "# Example: Check the first document\n",
    "if docs:\n",
    "    print(f\"Successfully loaded {len(docs)} movie plots.\")\n",
    "    print(\"--- Example Document (Nolan Movie) ---\")\n",
    "    print(docs[0])\n",
    "else:\n",
    "    print(\"No documents found matching the criteria (Nolan movies with a plot/title).\")\n",
    "\n",
    "vector_store_history.add_documents(docs)"
   ],
   "id": "fb8bee0479997e15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Standalone Question Generation\n",
    "\n",
    "Define a **prompt template** and a simple **processing chain** for rephrasing follow-up questions into standalone ones.\n",
    "\n",
    "\n",
    "When users ask context-dependent questions in a conversation, this step ensures each query can be understood independently of prior turns—useful for retrieval and context indexing.\n",
    "\n",
    "- **`standalone_system_prompt`** instructs the model to reformulate a follow-up question using the chat history, without answering it.\n",
    "- **`ChatPromptTemplate`** structures the prompt, combining:\n",
    "  - A system message with task instructions.\n",
    "  - A `MessagesPlaceholder` for inserting the chat history dynamically.\n",
    "  - The human message (`{question}`) containing the user’s latest input.\n",
    "- **`StrOutputParser`** converts the model’s output into a plain string.\n",
    "\n",
    "- The resulting `question_chain` pipes the prompt through the language model (`llm`) and parser, producing a clean standalone question ready for embedding or retrieval."
   ],
   "id": "fcf8483de872705f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# Create a prompt to generate standalone questions from follow-up questions\n",
    "standalone_system_prompt = \"\"\"\n",
    "Given a chat history and a follow-up question, rephrase the follow-up question to be a standalone question.\n",
    "Do NOT answer the question, just reformulate it if needed, otherwise return it as is.\n",
    "Only return the final standalone question.\n",
    "\"\"\"\n",
    "\n",
    "standalone_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", standalone_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "# Parse output as a string\n",
    "parse_output = StrOutputParser()\n",
    "\n",
    "question_chain = standalone_question_prompt | llm | parse_output"
   ],
   "id": "565d588dcbc79191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = [\n",
    "    (\"human\", \"What movies has Christopher Nolan directed?\"),\n",
    "    (\"ai\", \"Here’s a list of major Christopher Nolan–directed films: Inception, Interstellar, Oppenheimer\"),\n",
    "]\n",
    "followup_question = \"Who was the lead actor in those films?\"\n",
    "\n",
    "\n",
    "\n",
    "result = question_chain.invoke({\n",
    "    \"history\": history,\n",
    "    \"question\": followup_question\n",
    "})\n",
    "\n",
    "print(\"Standalone question:\")\n",
    "print(result)"
   ],
   "id": "b9e9d3a7d2460b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "history = [\n",
    "    (\"human\", \"I’m in the mood for a movie about space exploration, but not too scary or dark. Something exciting and maybe a little inspiring.\"),\n",
    "    (\"ai\", \"Here’s a list of movies 1. Wall-E (2008) – If you want something lighter, this animated film mixes space adventure with charm, heart, and environmental themes. 2. The Martian (2015) – A gripping survival story with humor and optimism as an astronaut is stranded on Mars. Very inspiring and scientifically grounded. 3. Gravity (2013) – Visually breathtaking survival story in space. A bit intense at times, but overall more thrilling than dark.\")\n",
    "]\n",
    "followup_question = \"I want to find only the movies released before 2020.\"\n",
    "\n",
    "result = question_chain.invoke({\n",
    "    \"history\": history,\n",
    "    \"question\": followup_question\n",
    "})\n",
    "\n",
    "print(\"Standalone question:\")\n",
    "print(result)"
   ],
   "id": "79692b7f67920066",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Session-Based Chat History\n",
    "\n",
    "Define a helper function to manage **session-specific chat memory** using MongoDB.\n",
    "\n",
    "The `get_session_history()` function returns a `MongoDBChatMessageHistory` object that connects to the `langchain_db.rag_with_memory` collection."
   ],
   "id": "86fd8cd242942af3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_mongodb.chat_message_histories import MongoDBChatMessageHistory\n",
    "\n",
    "def get_session_history(session_id: str) -> MongoDBChatMessageHistory:\n",
    "    return MongoDBChatMessageHistory(\n",
    "        connection_string=MONGO_CONNECTION_STRING,\n",
    "        session_id=session_id,\n",
    "        database_name=\"langchain_db\",\n",
    "        collection_name=\"rag_with_memory\"\n",
    "    )"
   ],
   "id": "adb764ade9894e67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "RunnablePassthrough.assign add additional keys to a chain.",
   "id": "63f3bcf11c40289c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vector_store_history.as_retriever(search_type=\"similarity\", search_kwargs={ \"k\": 5 })\n",
    "\n",
    "# Create a retriever chain that processes the question with history and retrieves documents\n",
    "retriever_chain = RunnablePassthrough.assign(context=question_chain | retriever | (lambda docs: \"\\n\\n\".join([d.page_content for d in docs])))"
   ],
   "id": "3fe4169aedcfc2b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a prompt template that includes the retrieved context and chat history\n",
    "rag_system_prompt = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", rag_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ],
   "id": "48bf88d3ae6dbe1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build the RAG chain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "rag_chain = (\n",
    "    retriever_chain\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | parse_output\n",
    ")\n",
    "\n",
    "# Wrap the chain with message history\n",
    "rag_with_memory = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ],
   "id": "95098417736a5131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First question\n",
    "response_1 = rag_with_memory.invoke(\n",
    "    {\"question\": \"Show me movies that tell the story of artificial intelligence becoming dangerous?\"},\n",
    "    {\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "print(response_1)"
   ],
   "id": "d6c44f0de36875bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Follow-up question that references the previous question\n",
    "response_2 = rag_with_memory.invoke(\n",
    "    {\"question\": \"Who was the lead actor in those films?\"},\n",
    "    {\"configurable\": {\"session_id\": \"user_1\"}}\n",
    ")\n",
    "print(response_2)"
   ],
   "id": "3b8a122cd8bbde30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "More details are to be found in:\n",
    "\n",
    "[Mongo docs - create embeddings](https://github.com/mongodb/docs-notebooks/blob/main/create-embeddings/openai-new-data.ipynb)\n",
    "[Mongo docs - LangChain](https://www.mongodb.com/docs/atlas/ai-integrations/langchain/get-started/#std-label-langchain-get-started)\n"
   ],
   "id": "e8cc698c5cfa4654"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
